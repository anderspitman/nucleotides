---
  title: Why?
---

New scientific software is created and released all the time. Software is
published as a article in a journal. These software articles usually contain
benchmarks and tests illustrating how the software improves over the existing
state of the art.

The problem is that most comparisons are subjective. Researchers are subject to
authorship bias and likely select the results that portray their software in
the best way. How can you compare two different pieces of scientific software
when their corresponding publications test them on two different data? The
current state of software publication in bioinformatics makes it almost
impossible for a researcher to objectively evaluate which software works best
for their own data.

The previous [Assemblathon 1][asm1] and [Assemblathon 2][asm2] projects aimed
to evaluate the current state of genome assembly. The method for their approach
was to release a set of reads and ask the genomics community to submit their
best assembly. The quality and accuracy of each of the submitted assemblies was
then evaluated. The [Genome Assembly Gold-Standard Evaluations (GAGE)][gage]
took several genome assemblers and ran them against four different read
datasets. The results of each assembler were then evaluated for performance.

This project aims to improve on these approaches in two ways:

  * Assembly benchmarks will be run on a weekly basis. This will allow the
    lastest developments and publications in genome assembly to be evaluated
    against the current corpus of assemblers.

  * The genome assemblers and pipelines will be submitted by the wider
    bioinformatics community itself. This will allow large numbers of
    assemblers to be evaluated simultaneously and without requiring manual
    installation or setting of parameters.

These two goals are made possible using [Linux Containers][lxc] via
[Docker][docker]. All genome assemblers and associated pipeline should be built
within a docker image and then hosted on [Docker Hub][hub]. Our benchmarking
pipeline will then pull the image and run it against an array of reference data
sets. The produced assembly is evaluated against the reference sequence using
[Quast][quast]. The assembly metrics and results are then posted on this site.


[asm1]: http://www.ncbi.nlm.nih.gov/pubmed/21926179

[asm2]: http://www.ncbi.nlm.nih.gov/pubmed/23870653

[gage]: http://www.ncbi.nlm.nih.gov/pubmed/22147368

[lxc]: https://linuxcontainers.org/

[docker]: http://www.docker.com/

[hub]: https://hub.docker.com/

[quast]: http://www.ncbi.nlm.nih.gov/pubmed/23422339
