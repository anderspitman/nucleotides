---
  title: October 2014 Update
---

This is the second update on recent changes to nucleotid.es. These include
additional assemblers and updates to existing assemblers. There are also
additional metrics added to benchmark the performance of each assembler. The
generated data are also now more accurate by using five replicates for each
benchmark.

### More assemblers

This project needs more assembler images. I created six assembler images
however there are many more assemblers that could be included. If you are
interested in creating Docker image please contact me through this mailing list
or through my personal email. Additional assembler images included here this
would extremely helpful and the more assemblers benchmarked the better the
picture of genome assembly this project provides.

This month was exciting for me because two assembler images were created by
others. Aaron Darling at the University of Technology Sydney created an image
of A5-miseq. Eugene Goltsman at the Joint Genome Institute made an image of
Meraculous. These assemblers have both been tested and you can view how these
assemblers perform in the benchmarks page. These results are particularly
interesting as both these assemblers perform well.

Shaun Jackman also provided feedback on the ABySS image. These comments came as
a [pull request][1] and on a [commit][2] and are useful for improving the
performance of the assembler image. The ABySS image now has an 'adaptive'
command bundle which uses [kmergenie][3] to search for the optimal kmer to use
for assembly.

[1]: https://github.com/nucleotides/docker-abyss/pull/2
[2]: https://github.com/nucleotides/docker-abyss/commit/8d841532bae4ba69bf65c82aedde9e5f449d41ea
[3]: http://kmergenie.bx.psu.edu/

The reason of nucleotid.es is provide accurate benchmarks for genome assemblers
that can be immediately used in your own work. Therefore if you are interested
in trying A5-miseq or Meraculous, then install Docker and you can start using
the images immediately. The same also applies if you would like to compare kmer
searching versus using a long kmer: you can use the ABySS 'adaptive' command
bundle and compare this with the 'k-96' command bundle and see how well each
performs for your own data.

### More metrics

I have added additional metrics to each benchmark. Each benchmark now includes
both local misassemblies and global misassemblies. These are useful for
providing detail on larger incorrect arrangements in to the already include
incorrect bases (IB) metric. All of these metrics on the benchmark page are
generated using [QUAST][4] by comparing the produced contigs with the reference
genome.

[4]: http://bioinf.spbau.ru/quast

The second set of metrics I have added relate to [Linux control groups][5].
These cgroups are used by the Docker daemon to organise the container processes
and contain information about memory and CPU usage. I collect these metrics
while each container is running by spawning a separate process that
periodically queries the cgroup for the running container. These metrics are
included in the benchmarks page and can be used to compare the computational
requirements for running each assembler.

[5]: https://www.kernel.org/doc/Documentation/cgroups/cgroups.txt

I further computed an additional metric: CPU seconds per assembled base. This
is the total number of CPU seconds used by the container divided by the total
length of the assembly. This metric provides a perspective on the efficiency of
each assembler, where a smaller number indicates a computationally more
efficient assembly.

### More replicates

Previously each assembler was benchmarked on a single FASTQ file from a
reference genome. This allowed the possibility that a benchmark could be over
fitted to the sampling of the reads. I have updated the benchmarks so that each
calculated metric is the result of running the assembler on five different
sampling of reads from the reference genome. This should therefore provide a
more accurate view of each how the assembler performs and I hope provide more
confidence in the results.
