---
  title: Why use containers for scientific software?
  date: !!timestamp 2015-01-07
  uri: post-4
---

Nucleotid.es benchmarking data has been available for seven months going from a
single table of results for one organism to the current ~1,900 replicated
benchmarks across multiple organisms. There has have been discussions on the
applicability of using containers for this kind of approach. In particular one
question is how does this lend to reproducibility in science? For example Titus
Brown wrote a blog post describing [a post-apocalyptic world of binary
containers][titus] and a [discussion started on twitter by Shaun
Jackman][twitter] lead to many replies. I also have also recieved similar
questions when I presented nucleotid.es.

[titus]: http://ivory.idyll.org/blog/2014-containers.html
[nucleotid.es]: http://nucleotid.es
[twitter]: https://twitter.com/sjackman/status/537723151057039362

### Reproducibility

If we specifically discuss Docker the implementation when we talk about
containers, and we almost certainly are, then I disagree with the description
of these as 'binary blobs' that cannot be understood. You can run the `docker
export` command to get a .tar of the container's file system. A Docker
container is not compiled in any way instead it is a series of transparent file
system layers. The act of containerising scientific software does not obscure
how the software works or make it inaccessible.

Instead I think that Docker containers make for more reproducible science. A
Dockerfile allows for the oppourtinity to explicitly show the steps to compile
and organise the code. This is better than simply providing the source code
alone and expecting the user to compile it. I can illustrate this with two
example Dockerfiles for genome assembly containers:

  * [velvet + kmergenie][velvet]
  * [idba][idba]

[velvet]: https://github.com/nucleotides/docker-velvet/blob/master/Dockerfile
[idba]: https://github.com/nucleotides/docker-idba/blob/master/Dockerfile

I hope that we can agree that neither are these trivial installs. The advantage
of using a container is that it saves everyone else from having to do this.
More importantly it saves them from having to **learn** how to do this. There
is a case for encouraging non-computational biologists to learn to code but not
for forcing them to debug g++ errors.

My favorite way to describe this is as "deduplicaton of agony". We can take the
pain of compiling and installing, often buggy and undocumented, bioinformatics
code which we force on our users and move that to into a container. Instead of
making everyone else do this work we can just ask the person who knows best to
do it: the developer.

### Interoperability

A second argument is that containers are 'black boxes' and cannot be used with
other tools. For instance if I can give you a container with a working version
of spades or ABySS. This will solve the problem of getting the software to
running but now you have to use it to produce results. This is the problem
nuclotid.es aims to solve.

I have taken some of the most popular genome assemblers and containerised them.
Importantly these have all been standardised behind the same interface so that
they all operate in the same way. This means that if you are using assembler X
and then new data suggests that assembler Y is better, you can immediately
switch the two containers because the have the same API. You can use all these
containers interchangably in your own custom pipelines with minimal development
effort.

This nucleotid.es project aims to provide the data to make the decisions about
which container to use. Furthermore because the software was tested inside
using container, the results are garanteed be the same for you as they were for
me when they were benchmarked. This would not be the case if there was not a
standardised interface as you would have to work out which parameters were
used.

### Standardisation

Of all the questions I am asked about nucleotid.es, by far the most common is
"How can I pass extra flags of arguments to the container?". For example can
you alter the kmer length for the genome assembler other than the default
included in the container?

The answer to this for nucleotid.es is that you can, and generally you
shouldn't want to first. First of all, if run the container from how it was
benchmarked, then you no longer can expect you results to follow the
benchmarks. If you change the container with customisation then this breaks the
contract of reproducibility. You can't just share the same container and the
problems occur with previous assemblathons where others can't reproduce the
results or workflows.

I believe that when someone asks this question what they really mean is that
they don't quite trust the container defaults and so would like to play with
the results to see if they can be improved. I think this is misguided however
as this means that not only is the tuning not reproducible but that every
assembly must be manually examined. Instead what must happen is that this
manual tinkering must be converted to an algorithm and then moved inside a
container. The container can then be benchmarked and effectiveness of this
being reproducibly examined.

### Summary

At the JGI we produce thousands of assemblies and terabases of sequence data
each year. The days where large sequencing centers manually improve genomes
have long passed, I believe as sequencing becomes cheaper and generated in ever
larger volumes this will be the case for most research labs. Nucleotid.es,
through the use of Docker containers, allows us to understand what kind of
results we might expect from a container and when someone inevitably produces a
better assembler we can identify it immediately and insert it into our
workflows.
